{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gyu\\anaconda3\\envs\\python-env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from transformers import pipeline, AutoTokenizer\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "from pysentimiento import create_analyzer\n",
    "import ijson\n",
    "import json\n",
    "import couchdb\n",
    "# from couchdb import *\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecimalEncoder(json.JSONEncoder):\n",
    "  def default(self, obj):\n",
    "    if isinstance(obj, Decimal):\n",
    "      return str(obj)\n",
    "    return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the sentiment analysis on each tweet\n",
    "def get_sentiment(sentiment_analyzer, tweet):\n",
    "    content = preprocess_tweet(tweet)\n",
    "    try:\n",
    "        sentiment = sentiment_analyzer.predict(content)\n",
    "    except:\n",
    "        pass\n",
    "    return sentiment.__dict__['output'], max(sentiment.__dict__['probas'].values())\n",
    "       \n",
    "        \n",
    "def couchify_tweet(tweet_json):\n",
    "    # keys_to_extract = [\"_id\", 'text', 'coordinates', 'geo', 'created_at', 'metadata', 'location', 'sentiment_label', 'sentiment_prob']\n",
    "    keys_to_exclude = [\"_rev\"]\n",
    "    json_couchified = {key: tweet_json[key] for key in keys_to_exclude if key not in keys_to_exclude}\n",
    "    if \"_id\" not in json_couchified:\n",
    "        json_couchified[\"_id\"] = json_couchified[\"id\"]\n",
    "    return json_couchified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "couchserver = couchdb.Server(\"http://dev:dev@172.26.131.7:5984/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbname = \"twitter_hist\"\n",
    "if dbname in couchserver:\n",
    "    db = couchserver[dbname]\n",
    "else:\n",
    "    db = couchserver.create(dbname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Database 'twitter_hist'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tupload tweets\n",
      "500 \tupload tweets\n"
     ]
    }
   ],
   "source": [
    "# need to filter for search keys & language before uploading?\n",
    "uploaded = 0\n",
    "with open(\"twitter-melb.json\", \"rb\") as f:\n",
    "    tweets_json = []\n",
    "    for i, record in enumerate(ijson.items(f, \"rows.item.doc\")):\n",
    "        if i % 500 == 0:\n",
    "            print(f'{i} \\tupload tweets')\n",
    "            tweets_json = [json.loads(tweet) for tweet in tweets_json]\n",
    "            tweets_json = [couchify_tweet(tweet) for tweet in tweets_json]\n",
    "            db.update(tweets_json)\n",
    "            tweets_json = []\n",
    "            uploaded += 500\n",
    "            if uploaded == 5000:\n",
    "                break\n",
    "        try:\n",
    "            tweet = record\n",
    "            tweet_text = preprocess_tweet(tweet['text'])\n",
    "            sentiment_label, sentiment_prob = get_sentiment(analyzer, tweet_text)\n",
    "            tweet['sentiment_label'] = sentiment_label\n",
    "            tweet['sentiment_prob'] = sentiment_prob\n",
    "            tweets_json.append(json.dumps(tweet, cls=DecimalEncoder))\n",
    "            # print(f'{i}: \\t{tweet_text}, \\n\\t{sentiment_label}, \\n\\t{sentiment_prob}')\n",
    "        except:\n",
    "            pass\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86132030c22f4560eeb593796c7a3aeacc840788af4f511ae49e43b2847bdc45"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('python-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
